import React, { useRef, useState } from 'react';
import { View, Text, Pressable, StyleSheet, Image } from 'react-native';
import {
  Camera,
  useCameraDevice,
  useCameraPermission,
  TorchMode,
} from 'react-native-vision-camera';
import { useFrameProcessor } from 'react-native-vision-camera'
// import moveToGallery from './fileCopy';



const App = () => {
  const { hasPermission, requestPermission } = useCameraPermission();
  //hook provided by module to check permmission state and also provides method named requestPermission 
  //to pop up setting  
  console.log(hasPermission)
  const device = useCameraDevice('back');
  console.log("the returned device we got",device)
  console.log("device is an object which has properties",device?.minZoom)
  // to access best camera available on device
  const cameraRef = useRef<Camera>(null);// this stays consistent across re-renders
  // useRef is a hook that gives you a container (called a "ref") that
  //  can store a value without causing a re-render when it changes.
  // "It's a special container that remembers a value between renders but 
  // changing it doesn’t cause a re-render."
  const [photoUri, setPhotoUri] = useState("");


// function printCentreHexValue([r,g,b]){
//     return "#" + [r, g, b].map(x => {
//     const hex = x.toString(16);
//     return hex.length === 1 ? "0" + hex : hex;
//   }).join("");

// }



  const captureImage = async () => {
    if (cameraRef.current) {
      const photo = await cameraRef.current.takePhoto({
        flash: 'on',
        //returned photo is an  object//
      });
      setPhotoUri(`file://${photo.path}`);
      console.log("this is the photo objet we got",photo)
      // moveToGallery(photo.path)
      // { isMirrored: false,
      //                        │ path: '/data/user/0/com.privacy/cache/mrousavy8432853042043491865.jpg',
      //                        │ isRawPhoto: false,
      //                        │ height: 4080,
      //                        │ orientation: 'portrait',
      //                        └ width: 3060 }
    }
  };


const frameProcessor = useFrameProcessor((frame) => {
  'worklet';

  const format = frame.pixelFormat;
  console.log("this is the format",format)
  const width = frame.width;
  const height = frame.height;

  const centerX = Math.floor(width / 2);
  const centerY = Math.floor(height / 2);

  let bytesPerPixel;
  if (format === 'rgba') {
    bytesPerPixel = 4;
  } else if (format === 'rgb') {
    bytesPerPixel = 3;
  } else {
    console.log(`Unsupported pixel format: ${format}`);
    return;
  }

  try {
    const buffer = frame.toArrayBuffer();
    if (!buffer) {
      console.log('Empty buffer received');
      return;
    }

    const data = new Uint8Array(buffer);

    console.log("Width:", width);
    console.log("Height:", height);
    console.log("Pixel Format:", format);
    console.log("Bytes per pixel:", bytesPerPixel);
    console.log("Data length:", data.length);

    const index = (centerY * width + centerX) * bytesPerPixel;
    console.log("Calculated index:", index);
    console.log("Max valid index:", data.length - bytesPerPixel);

    if (index > data.length - bytesPerPixel) {
      console.log('Index out of bounds, clamping index');
    }

    const safeIndex = Math.min(index, data.length - bytesPerPixel);

    const r = data[safeIndex];
    const g = data[safeIndex + 1];
    const b = data[safeIndex + 2];

    function rgbToHex(r, g, b) {
      return (
        '#' +
        [r, g, b]
          .map((x) => x.toString(16).padStart(2, '0'))
          .join('')
      );
    }

    const hexColor = rgbToHex(r, g, b);

    console.log(`Center pixel RGB: (${r}, ${g}, ${b}), Hex: ${hexColor}`);
  } catch (error) {
    console.log('Error processing frame:', error);
  }
}, []);




// console.log("this is the frame processor",frameProcessor)








  if (device == null) return <View><Text>Loading...</Text></View>;

  if (!hasPermission) {
    return (
      <View style={styles.container}>
        <Text style={styles.text}>Camera permission needed</Text>
        <Pressable onPress={()=>requestPermission()} style={styles.button}>
          <Text style={styles.buttonText}>Grant</Text>
        </Pressable>
      </View>
    );
  }

  return (
    <View style={styles.container}>
      {photoUri ? (
        <>
          <Image
            source={{ uri: photoUri }}
            style={{ width: '100%', height: '80%' }}
            resizeMode="cover"
          />
          <Pressable style={styles.button} onPress={() => setPhotoUri("")}>
            <Text style={styles.buttonText}>Back to Camera</Text>
          </Pressable>
        </>
      ) : (
       
        <>
      
          <Camera
            style={styles.camera}
            device={device}
            pixelFormat="rgb" // 👈 Force RGB format
              frameProcessor={frameProcessor}  // ✅ this line is required
              // here frameprocesser will be automatically passed frame argument
            isActive={true}
            // Whether the Camera should actively stream video frames
            ref={cameraRef}
            //  torch={TorchMode:'on'}
            resizeMode={"contain"}
            preview={true}
            // Preview = The live camera feed you see on screen.
            // That visible "live image" — the real-time stream from your camera sensor — is called the preview.
            photo={true} // very important!
             onPreviewStarted={() => console.log('Preview started!')}
               onPreviewStopped={() => console.log('Preview stopped!')}
              //  Preview frame rate is the number of frames (images) the camera sends per second to render the live preview on screen.
              // fps={15}
          />
          <Pressable style={styles.captureButton} onPress={captureImage}>
            <Text style={styles.buttonText}>Click Photo</Text>
          </Pressable>
        </>
      )}
    </View>
  );
};

export default App;

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
    justifyContent: 'center',
    alignItems: 'center',
  },
  text: { color: '#fff', marginBottom: 20 },
  button: {
    backgroundColor: '#007bff',
    padding: 10,
    marginTop: 20,
    borderRadius: 6,
  },
  captureButton: {
    position: 'absolute',
    bottom: 40,
    backgroundColor: '#1abc9c',
    padding: 14,
    borderRadius: 50,
  },
  buttonText: { color: 'white', fontWeight: 'bold' },
  camera: {
    width: '100%',
    height: '100%',
  },
});



















// can't we do full data console log, but it will disrupt flow inse that would be too large array
// ChatGPT said:

// You're absolutely right — and this is an excellent observation.
// ✅ Yes, You can console.log() the full pixel data (Uint8Array), but:

//     It's a huge array:
//     A 1080×1920 image (Full HD) has:

//         1080 × 1920 × 3 = 6,220,800 values (for RGB)

//         That’s over 6 million integers between 0–255.

//     Printing it will freeze or crash your app:
//     console.log(hugeArray) will block the JS thread, flood the Metro console, and may cause:

//         Frame drops / crashes

//         Metro Bundler lag or hang

//         Device performance issues

// ✅ Best Practice

// Instead of logging the full array, log just a small portion:

// console.log('First 10 RGB values:', data.slice(0, 10))

// Or if you want to log the first 3 pixels:

// for (let i = 0; i < 3 * 3; i += 3) {
//   console.log(`Pixel ${i / 3}: RGB(${data[i]}, ${data[i + 1]}, ${data[i + 2]})`)
// }

// 🧪 For Debugging: Save to file instead of console

// If you're deeply testing the RGB array and need all values:

//     Write data to a .json file or use react-native-fs to save to storage.

//     Or send the buffer to a server endpoint for inspection.

// Summary
// ❌ Don't	✅ Do Instead
// console.log(data) if it's huge	Log only data.slice(0, 10)
// console.log(frame)	Use frame.toString()
// Use frame after async	Copy values first, then use

// Let me know if you'd like help writing a debug logger that prints specific pixels like center, top-left, or average color!
